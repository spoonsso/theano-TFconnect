{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to:\n",
    "* get data from the kaggle connectomics competition into a suitable form for processing by TensorFlow\n",
    "* construct a network following the published deep conv net architecture. aim to reproduce the published AUC\n",
    "* improve the power of the model with newer network architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "#\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, accuracy_score, roc_auc_score\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I/O functions provided by competition \n",
    "# https://github.com/bisakha/Connectomics/\n",
    "#\n",
    "def iter_loadtxt(filename, delimiter=',', skiprows=0, dtype=float):\n",
    "    def iter_func():\n",
    "        with open(filename, 'r') as infile:\n",
    "            for _ in range(skiprows):\n",
    "                next(infile)\n",
    "            for line in infile:\n",
    "                line = line.rstrip().split(delimiter)\n",
    "                for item in line:\n",
    "                    yield dtype(item)\n",
    "        iter_loadtxt.rowlength = len(line)\n",
    "\n",
    "    data = np.fromiter(iter_func(), dtype=dtype)\n",
    "    data = data.reshape((-1, iter_loadtxt.rowlength))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I/O functions provided by competition \n",
    "# https://github.com/bisakha/Connectomics/\n",
    "#\n",
    "def loadnet(filename):\n",
    "    l1 = np.loadtxt(filename,delimiter=\",\")    \n",
    "\n",
    "    #% Keep only 0/1 weights, ignore blocked connections\n",
    "    Matrix = [[0 for x in range(int(l1.max()))] for x in range(int(l1.max()))]\n",
    "\n",
    "    for i in range(0,len(l1)-1):\n",
    "        if l1[i][2] > 0:\n",
    "            l1[i][2] = 1\n",
    "\n",
    "            Matrix[int(l1[i][0])-1][int(l1[i][1])-1] = 1\n",
    "\n",
    "        if l1[i][2] <= 0:\n",
    "            l1[i][2] = 0\n",
    "\n",
    "            Matrix[int(l1[i][0])-1][int(l1[i][1])-1] = 0\n",
    "\n",
    "    return np.array(Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 14s\n"
     ]
    }
   ],
   "source": [
    "# Read training dataset\n",
    "filename = '../Tensor/kaggle_connect_data/normal-1/fluorescence_normal-1.txt'\n",
    "%time F = iter_loadtxt(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 14s\n"
     ]
    }
   ],
   "source": [
    "# Read validation dataset\n",
    "filename = '../Tensor/kaggle_connect_data/normal-2/fluorescence_normal-2.txt'\n",
    "%time F_valid = iter_loadtxt(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read training connectivity matrix\n",
    "filename = '../Tensor/kaggle_connect_data/normal-1/network_normal-1.txt'\n",
    "scores = loadnet(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read validation connectivity matrix\n",
    "filename = '../Tensor/kaggle_connect_data/normal-2/network_normal-2.txt'\n",
    "scores_valid = loadnet(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, we want to create an \"image set\" of all possible 2-permutations of the data, + the average activity of the network. This will form our first tensor.\n",
    "\n",
    "But Romaszko also downsampled the data using an activity threshold -- we should do the same downsampling first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def roma_ds(fluor):\n",
    "    \"\"\"\n",
    "    Takes a 2-D numpy array of fluorescence time series (in many cells) and returns a downsampled version, following the\n",
    "        filtering method published by Romaszko (threshold summed time-diff global network activity at 0.02)\n",
    "\n",
    "    inputs---\n",
    "        fluor: numpy array of fluorescence time series. rows are cells, columns are time points / frames\n",
    "    outputs---\n",
    "        fluor_ds: downsampled numpy array of fluorescence time series.\n",
    "    \"\"\"\n",
    "    thresh = 20\n",
    "    fluordiff = np.diff(fluor, axis=1)\n",
    "    totF = np.sum(fluordiff, axis=0)\n",
    "    fluor = fluor[:,totF>thresh]\n",
    "    \n",
    "    return fluor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dunn_ds(fluor):\n",
    "    \"\"\"\n",
    "    Takes a 2-D numpy array of fluorescence time series (in many cells) and returns a \n",
    "    downsampled version by taking every 100th frame\n",
    "\n",
    "    inputs---\n",
    "        fluor: numpy array of fluorescence time series. rows are cells, columns are time points / frames\n",
    "    outputs---\n",
    "        fluor_ds: downsampled numpy array of fluorescence time series.\n",
    "    \"\"\"\n",
    "    \n",
    "    return np.diff(fluor[:,::10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the training tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pairwise_prep(fluor, connect):\n",
    "    \"\"\"\n",
    "    Takes a 2-D numpy array of fluorescence time series (in many cells) and returns a 4-D numpy array ready for tensorflow,\n",
    "        with each \"image\" constructed from a pair of fluorescence traces + average network activity. \n",
    "        \n",
    "    This tensor incorporates several features from the Romaszko setup.\n",
    "        1) fragments of length 330 frames are extracted, at random start positions, from each fluorescence trace\n",
    "        2) equal representation of connected and non-connected pairs\n",
    "        3) 1.2 million total examples\n",
    "        4) equal representation of any included pairs -- meaning no connected pair, if included, is represented any more than\n",
    "            another\n",
    "        \n",
    "    While at it, builds a structure of one-hot vectors (in this case, a single binary label) \n",
    "        to signify \"connected\" or \"not connected\"\n",
    "    \n",
    "    inputs---\n",
    "        fluor: 2-D numpy array of fluorescence time series. rows are cells, columns are time points / frames\n",
    "        connect: 2-D numpy array connectivity matrix summarizing all possible pairwise connectivity.\n",
    "    outputs---\n",
    "        fluor_tf: 4-D pairwise numpy array ready for tensorflow\n",
    "        label_tf: a 1-D numpy array labeling connectivity for each possible pair in the dataset\n",
    "    \"\"\"\n",
    "    num_cells = fluor.shape[0]\n",
    "    num_samples = 330\n",
    "    \n",
    "    raw_samples = fluor.shape[1]\n",
    "    \n",
    "    # In theory cells could be auto-connected\n",
    "    num_images_target = 1.2e6\n",
    "    \n",
    "    # Find all connected pairs\n",
    "    cons = np.where(connect==1)\n",
    "    num_con = len(cons[0])\n",
    "    num_con_reps = np.floor(num_images_target/2/num_con).astype('int')\n",
    "    \n",
    "    num_images = num_con_reps*num_con*2\n",
    "    fluor_tf = np.empty((num_images, 3, num_samples, 1),dtype='float32')\n",
    "    label_tf = np.zeros((num_images,2),dtype='float32')\n",
    "    \n",
    "    avg_F = np.mean(fluor,axis=0)\n",
    "    \n",
    "    # Add conncted pairs to tensor\n",
    "    cnt = 0\n",
    "    for i in range(num_con):\n",
    "        for j in range(num_con_reps):\n",
    "            startpos = np.random.randint(0,raw_samples-num_samples,1)[0]\n",
    "            fluor_tf[cnt,:,:,0] = np.vstack((fluor[cons[0][i],startpos:startpos+num_samples], \n",
    "                                                   fluor[cons[1][i],startpos:startpos+num_samples], \n",
    "                                                   avg_F[startpos:startpos+num_samples]))\n",
    "            label_tf[cnt,0] = 1\n",
    "            cnt += 1\n",
    "    \n",
    "    # Find all non-connected pairs\n",
    "    # There are typically too many non-connected pairs to have any repetitions in the training set\n",
    "    noncons = np.where(connect==0)\n",
    "    \n",
    "    # Sample randomly from noncons without replacement\n",
    "    noncons_samp = (np.random.choice(noncons[0],num_images/2,replace=False), \n",
    "                    np.random.choice(noncons[1],num_images/2,replace=False))\n",
    "    \n",
    "    # Create final data structure\n",
    "    for i in range(num_images//2):\n",
    "        startpos = np.random.randint(0,raw_samples-num_samples,1)[0]\n",
    "        fluor_tf[cnt,:,:,0] = np.vstack((fluor[noncons_samp[0][i],startpos:startpos+num_samples], \n",
    "                                               fluor[noncons_samp[1][i],startpos:startpos+num_samples], \n",
    "                                               avg_F[startpos:startpos+num_samples]))\n",
    "        label_tf[cnt,1] = 1\n",
    "        cnt += 1\n",
    "            \n",
    "            \n",
    "    return fluor_tf, label_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tim\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:14: VisibleDeprecationWarning: boolean index did not match indexed array along dimension 1; dimension is 179500 but corresponding boolean dimension is 179499\n"
     ]
    }
   ],
   "source": [
    "# Downsample fluorescence time series using a hard population activity threshold \n",
    "#\n",
    "ds = roma_ds(F.T)\n",
    "ds_valid = roma_ds(F_valid.T)\n",
    "F = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1856)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standardize training data\n",
    "#\n",
    "vs = ds - np.mean(ds,axis=1)[:,None]\n",
    "vs = vs/np.std(vs,axis=1)[:,None]\n",
    "vs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1553)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standardize validation data\n",
    "#\n",
    "vs_valid = ds_valid - np.mean(ds_valid,axis=1)[:,None]\n",
    "vs_valid = vs_valid/np.std(vs_valid,axis=1)[:,None]\n",
    "vs_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tim\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:61: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "C:\\Users\\Tim\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:62: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "# Prepare trainign data structure + labels\n",
    "#\n",
    "dtf, ltf = pairwise_prep(vs,scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1194900, 3, 330, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1194900, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ltf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle everything and save 4% as a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1194900, 3, 330, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now we can set up our network layers using tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standard 2-D convolution\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='VALID')\n",
    "\n",
    "# Only used in the last layer\n",
    "def max_pool_1x10(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 1, 10, 1],\n",
    "                        strides=[1, 1, 10, 1], padding='VALID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None,dtf.shape[1],dtf.shape[2],1])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_conv1 = weight_variable([2, 5, 1, 18])\n",
    "b_conv1 = bias_variable([18])\n",
    "\n",
    "h_conv1 = tf.nn.tanh(conv2d(x, W_conv1) + b_conv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_conv2 = weight_variable([2, 5, 18, 40])\n",
    "b_conv2 = bias_variable([40])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_conv1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_1x10(h_conv2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_conv3 = weight_variable([1, 1, 40, 15])\n",
    "b_conv3 = bias_variable([15])\n",
    "\n",
    "h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W_fc1 = weight_variable([32 * 1 * 15, 100])\n",
    "b_fc1 = bias_variable([100])\n",
    "\n",
    "h_conv3_flat = tf.reshape(h_conv3, [-1, 32 * 1 * 15])\n",
    "h_fc1 = tf.nn.tanh(tf.matmul(h_conv3_flat, W_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Readout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_fc2 = weight_variable([100, 2])\n",
    "b_fc2 = bias_variable([2])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def valid_eval(session, val_dat, val_lbl, N=14, fragLen=330):\n",
    "    \"\"\"\n",
    "    Properly calidates current CNN filters by passing filters over retained validation set N number of times and averaging\n",
    "    the set of predictiosn for each pair\n",
    "    \n",
    "    inputs---\n",
    "        session: tensorflow session object\n",
    "        val_dat: 2-D numpy array of downsampled fluorescence traces\n",
    "        val_lbl: 2-D numpy array of connectivity labels for eah pair (connectivity matrix)\n",
    "        N: number of separate start positions for each test fragment to be averaged for each pair\n",
    "        fragLen: length of trained CNN filter, in time points/samples\n",
    "    outputs---\n",
    "        AUC for the validated predictions\n",
    "    \"\"\"\n",
    "    avg_F = np.mean(val_dat,axis=0)\n",
    "\n",
    "    startgap = np.ceil((val_dat.shape[1] - fragLen)/N).astype('int')\n",
    "    true_lbl = np.zeros((val_dat.shape[0]*val_dat.shape[0],), dtype='float32')\n",
    "    pred_lbl = np.zeros((val_dat.shape[0]*val_dat.shape[0],), dtype='float32')\n",
    "    \n",
    "    # Counter for the \"true_lbl\" array\n",
    "    cnt_ = 0\n",
    "    # Counter for the \"pred_lbl\" array\n",
    "    cnt_u = 0\n",
    "    for a in range(val_dat.shape[0]):\n",
    "        if a%100 == 0:\n",
    "            print('\\r' + 'X'*(a//100))\n",
    "        \n",
    "        # Create batch array to send thru network\n",
    "        im_eval = np.empty((N*val_dat.shape[0],3,fragLen,1), dtype='float32')\n",
    "        \n",
    "        # Count the number of traces in each batch\n",
    "        cnt = 0\n",
    "            \n",
    "        for b in range(val_dat.shape[0]):\n",
    "            \n",
    "            for n in range(0, val_dat.shape[1] - fragLen, startgap):\n",
    "                try:\n",
    "                    im_eval[cnt,:,:,0] = np.vstack((val_dat[a,n:n+fragLen],\n",
    "                                         val_dat[b,n:n+fragLen],\n",
    "                                         avg_F[n:n+fragLen]))\n",
    "                except:\n",
    "                    from IPython.core.debugger import Tracer\n",
    "                    Tracer()()\n",
    "    \n",
    "                cnt += 1\n",
    "            \n",
    "            # Keep track of the true labels\n",
    "            if val_lbl[a,b] == 1:\n",
    "                true_lbl[cnt_] = 1\n",
    "            else:\n",
    "                true_lbl[cnt_] = 0\n",
    "            \n",
    "            cnt_ += 1\n",
    "        \n",
    "        # Run batch through network\n",
    "        pred_stop = sess.run(dispense_data, feed_dict={x: im_eval })[:,0]\n",
    "        # Average output over each group of N traces\n",
    "        for u in range(0, len(pred_stop), N):\n",
    "            pred_lbl[cnt_u] = np.mean(pred_stop[u:u+N])\n",
    "            cnt_u += 1        \n",
    "    \n",
    "    # Get ROC metrics\n",
    "    fpr, tpr, thresholds = roc_curve(true_lbl, pred_lbl)\n",
    "    \n",
    "    return auc(fpr, tpr)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate.\n",
    "\n",
    "In theory we should be using a negative log-likelihood cost function (in order to match Romaszko exactly), but we'll go with a standard cross entropy here.\n",
    "\n",
    "Also, Romaszko used a gradient descent optimizer with momentum -- we can change this later too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "step 0, training accuracy 0.899445\n",
      "loss: 0.3882453933511581\n",
      "\n",
      "X\n",
      "XX\n",
      "XXX\n",
      "XXXX\n",
      "XXXXX\n",
      "XXXXXX\n",
      "XXXXXXX\n",
      "XXXXXXXX\n",
      "XXXXXXXXX\n",
      "Batch validation score: 0.9209667420044925\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "step 1, training accuracy 0.915215\n",
      "loss: 0.3615640078252185\n",
      "\n",
      "X\n",
      "XX\n",
      "XXX\n",
      "XXXX\n",
      "XXXXX\n",
      "XXXXXX\n",
      "XXXXXXX\n",
      "XXXXXXXX\n",
      "XXXXXXXXX\n",
      "Batch validation score: 0.9264413391322919\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "step 2, training accuracy 0.918206\n",
      "loss: 0.3561718918926217\n",
      "\n",
      "X\n",
      "XX\n",
      "XXX\n",
      "XXXX\n",
      "XXXXX\n",
      "XXXXXX\n",
      "XXXXXXX\n",
      "XXXXXXXX\n",
      "XXXXXXXXX\n",
      "Batch validation score: 0.9291227168313412\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "step 3, training accuracy 0.919938\n",
      "loss: 0.35272164453132465\n",
      "\n",
      "X\n",
      "XX\n",
      "XXX\n",
      "XXXX\n",
      "XXXXX\n",
      "XXXXXX\n",
      "XXXXXXX\n",
      "XXXXXXXX\n",
      "XXXXXXXXX\n",
      "Batch validation score: 0.930173425336805\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "step 4, training accuracy 0.921137\n",
      "loss: 0.3509770219161806\n",
      "\n",
      "X\n",
      "XX\n",
      "XXX\n",
      "XXXX\n",
      "XXXXX\n",
      "XXXXXX\n",
      "XXXXXXX\n",
      "XXXXXXXX\n",
      "XXXXXXXXX\n",
      "Batch validation score: 0.9311405034283188\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "step 5, training accuracy 0.922086\n",
      "loss: 0.34905731363749043\n",
      "\n",
      "X\n",
      "XX\n",
      "XXX\n",
      "XXXX\n",
      "XXXXX\n",
      "XXXXXX\n",
      "XXXXXXX\n",
      "XXXXXXXX\n",
      "XXXXXXXXX\n",
      "Batch validation score: 0.9325794931292686\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "step 6, training accuracy 0.923023\n",
      "loss: 0.3473479591739637\n",
      "\n",
      "X\n",
      "XX\n",
      "XXX\n",
      "XXXX\n",
      "XXXXX\n",
      "XXXXXX\n",
      "XXXXXXX\n",
      "XXXXXXXX\n",
      "XXXXXXXXX\n",
      "Batch validation score: 0.9326967857631873\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "step 7, training accuracy 0.924002\n",
      "loss: 0.3455347363964186\n",
      "\n",
      "X\n",
      "XX\n",
      "XXX\n",
      "XXXX\n",
      "XXXXX\n",
      "XXXXXX\n",
      "XXXXXXX\n",
      "XXXXXXXX\n",
      "XXXXXXXXX\n",
      "Batch validation score: 0.9331720287848614\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "step 8, training accuracy 0.924987\n",
      "loss: 0.34362274671720255\n",
      "\n",
      "X\n",
      "XX\n",
      "XXX\n",
      "XXXX\n",
      "XXXXX\n",
      "XXXXXX\n",
      "XXXXXXX\n",
      "XXXXXXXX\n",
      "XXXXXXXXX\n",
      "Batch validation score: 0.9339308162190502\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "step 9, training accuracy 0.925924\n",
      "loss: 0.3413804002522085\n",
      "\n",
      "X\n",
      "XX\n",
      "XXX\n",
      "XXXX\n",
      "XXXXX\n",
      "XXXXXX\n",
      "XXXXXXX\n",
      "XXXXXXXX\n",
      "XXXXXXXXX\n",
      "Batch validation score: 0.9343229357936254\n"
     ]
    }
   ],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_conv, y_))\n",
    "train_step = tf.train.AdamOptimizer(1e-3).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "\n",
    "dispense_data = tf.nn.softmax(y_conv)\n",
    "\n",
    "collect_prediction = tf.reduce_mean(y_conv,0)\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "num_epochs = 10\n",
    "num_batches = dtf.shape[0]/1000\n",
    "\n",
    "for j in range(num_epochs):\n",
    "    inds = np.random.choice(dtf.shape[0],replace=False,size=dtf.shape[0])\n",
    "    dtf = dtf[inds,:,:,:]\n",
    "    ltf = ltf[inds]\n",
    "    score = 0\n",
    "    loss = 0\n",
    "    \n",
    "    for i in range(0,dtf.shape[0],1000):\n",
    "\n",
    "        \n",
    "        im = dtf[i:i+1000,:,:,:]\n",
    "        lbl = ltf[i:i+1000,:]#.astype('int')\n",
    "\n",
    "        train_step.run(feed_dict={x: im, y_: lbl})\n",
    "    \n",
    "        fpr, tpr, thresholds = roc_curve(lbl[:,0], dispense_data.eval(feed_dict={x: im})[:,0])\n",
    "        score += auc(fpr, tpr)\n",
    "        \n",
    "        loss += cross_entropy.eval(feed_dict={x: im, y_: lbl})\n",
    "    \n",
    "        if i%100000 == 0:\n",
    "            print(i)\n",
    "            \n",
    "\n",
    "    print(\"step %d, training accuracy %g\"%(j, score/num_batches))\n",
    "    print(\"loss: {}\".format(loss/num_batches))\n",
    "    \n",
    "    real = valid_eval(sess, vs_valid, scores_valid)\n",
    "    print(\"Batch validation score: {}\".format(real))\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
